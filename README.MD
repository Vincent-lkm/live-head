# üì° live_head

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Cloudflare Workers](https://img.shields.io/badge/Cloudflare-Workers-orange.svg)](https://workers.cloudflare.com/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-Compatible-blue.svg)](https://kubernetes.io/)

> **Monitoring HTTP haute performance pour 30 000 sites WordPress** - Surveillance distribu√©e avec alertes en temps r√©el

## üìã Table des mati√®res

- [‚ú® Fonctionnalit√©s](#-fonctionnalit√©s)
- [üèóÔ∏è Architecture](#Ô∏è-architecture)
- [‚ö° Installation rapide](#-installation-rapide)
- [üåê API Endpoints](#-api-endpoints)
- [ü§ñ Configuration des Pods](#-configuration-des-pods)
- [‚è±Ô∏è Planification (CronJob)](#Ô∏è-planification-cronjob)
- [üóÑÔ∏è Base de donn√©es (D1)](#Ô∏è-base-de-donn√©es-d1)
- [üìä Exemples de requ√™tes SQL](#-exemples-de-requ√™tes-sql)
- [üõ†Ô∏è Maintenance](#Ô∏è-maintenance)
- [üí∞ Co√ªts](#-co√ªts)
- [üîß Troubleshooting](#-troubleshooting)

## ‚ú® Fonctionnalit√©s

- [x] **Monitoring distribu√©** - Surveillance de 30k+ sites WordPress via pods Kubernetes
- [x] **D√©tection fiable** - GET avec range (36% plus rapide que HEAD, d√©tecte vraiment les erreurs)
- [x] **Alertes en temps r√©el** - Discord/Slack/webhook pour erreurs 5xx et redirections cross-domain
- [x] **Stockage optimis√©** - Cloudflare D1 avec historique configurable
- [x] **API REST compl√®te** - Endpoints pour consultation et int√©gration
- [x] **Faible co√ªt** - ~5-10$/mois pour 30k sites (scan toutes les 30min)
- [x] **M√©triques d√©taill√©es** - Latence, codes HTTP, redirections, ports

## üèóÔ∏è Architecture

```mermaid
graph TB
    A[Pods Apache K8s] --> B[live_head.sh]
    B --> C[Cloudflare Worker]
    C --> D[D1 Database]
    C --> E[Alertes Discord/Slack]
    F[Collector externe] --> C
    F --> G[Base SQL interne]
    G --> H[Dashboards/Grafana]
    
    A --> A1[Pod 1<br/>~150-200 sites]
    A --> A2[Pod 2<br/>~150-200 sites]
    A --> A3[Pod N<br/>~150-200 sites]
    
    D --> D1[health_last<br/>√âtat actuel]
    D --> D2[health_history<br/>Historique]
```

### Composants

| Composant | R√¥le | Co√ªt |
|-----------|------|------|
| **Pods Apache K8s** | H√©bergement ~150-200 sites WP chacun | Inclus infra |
| **live_head.sh** | Script de scan (GET avec range, normalise 206‚Üí200) | - |
| **Cloudflare Worker** | API + alertes + stockage | $5/mois |
| **D1 Database** | Stockage √©tats + historique | Inclus |
| **Collector externe** | Extraction donn√©es pour BI | - |

### M√©triques collect√©es

- ‚úÖ **Code HTTP** (200, 301, 404, 500...)
- ‚úÖ **Latence** (millisecondes)
- ‚úÖ **Port** de r√©ponse
- ‚úÖ **Redirections** avec d√©tection cross-domain
- ‚úÖ **Timestamp** pr√©cis

## ‚ö° Installation rapide

### 1. D√©ploiement Cloudflare

```bash
# Appliquer les migrations D1
wrangler d1 migrations apply live-head-db --remote

# D√©ployer le Worker
wrangler deploy
```

### 2. Test de fonctionnement

```bash
# V√©rifier le Worker
curl -s https://<worker>/health

# Test d'envoi de donn√©es
TOKEN="mon_token_secret"
curl -sX POST https://<worker>/push \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"site":"site.com","status":301,"ms":12,"port":8080,"location":"https://www.site.com/","ts":1735647382000}'

# Lecture du dernier √©tat
curl -s "https://<worker>/last?site=site.com" | jq
```

> ‚úÖ Si tout fonctionne, vous devriez voir `{"ok":true}` pour health et des donn√©es pour le test.

## üåê API Endpoints

### `GET /health`
**V√©rification de sant√© du Worker**
```bash
curl -s https://<worker>/health
# R√©ponse: {"ok": true}
```

---

### `POST /push`
**Envoi de m√©triques (1 mesure ou array)**

**Headers requis :**
- `Authorization: Bearer <token>`
- `Content-Type: application/json`

**Payload (mesure unique) :**
```json
{
  "site": "example.com",
  "status": 200,
  "ms": 18,
  "port": 8080,
  "ts": 1735648000000
}
```

**Payload (batch) :**
```json
[
  {"site":"site1.com","status":200,"ms":18,"port":8080,"ts":1735648000000},
  {"site":"site2.com","status":301,"ms":12,"port":8080,"location":"https://www.site2.com/","ts":1735648010000}
]
```

---

### `GET /last`
**Dernier √©tat d'un site sp√©cifique**

```bash
curl -s "https://<worker>/last?site=example.com" | jq
```

**R√©ponse :**
```json
{
  "ok": true,
  "site": "example.com",
  "data": {
    "s": 301,     // status
    "ms": 12,     // latence
    "p": 8080,    // port
    "l": "https://www.example.com/",  // location (si redirect)
    "x": 0,       // cross-domain flag (0=non, 1=oui)
    "t": 1735647382000  // timestamp
  }
}
```

---

### `GET /dump_last`
**Export complet des derniers √©tats**

#### Param√®tres de filtrage

| Param√®tre | Description | Exemple |
|-----------|-------------|---------|
| `status` | Codes HTTP multiples | `status=500,403,301` |
| `cross` | Redirections cross-domain | `cross=1` ou `cross=true` |
| `since` | Depuis timestamp (ms) | `since=1735647382000` |
| `until` | Jusqu'√† timestamp (ms) | `until=1735747382000` |
| `site_like` | Filtre SQL LIKE | `site_like=%25example.com` |
| `limit` | Nombre max (d√©faut: 1000, max: 5000) | `limit=200` |
| `offset` | D√©calage pagination | `offset=1000` |
| `order` | Ordre tri (asc/desc, d√©faut: desc) | `order=asc` |

#### Exemples d'usage

```bash
# Tous les 500/403/301 r√©cents
curl -s "https://<worker>/dump_last?status=500,403,301&limit=200" | jq

# Uniquement les redirections cross-domain
curl -s "https://<worker>/dump_last?cross=1" | jq

# Sites contenant "example.com"
curl -s "https://<worker>/dump_last?site_like=%25example.com" | jq

# Erreurs depuis hier
YESTERDAY=$(date -d "yesterday" +%s)000
curl -s "https://<worker>/dump_last?status=500,502,503&since=${YESTERDAY}" | jq
```

## ü§ñ Configuration des Pods

### Script `/mnt/www/update/live_head.sh`

Cr√©er le fichier suivant sur chaque pod :

```bash
#!/usr/bin/env bash

# Configuration
ROOT="/mnt/www"
PORT="8080"
LOCAL_IP="$(hostname -i | awk '{print $1}')"
WORKER_URL="https://live-head.restless-dust-dcc3.workers.dev/push"
AUTH_TOKEN="oROyVhmcMQ3W1ZjGs3CV798g0UJ9I4kHEulDgb76Tzru8t"

POD_INFO=$(basename /mnt/www/update/infra*-group* 2>/dev/null | head -n1)
POD_INFO="${POD_INFO:-unknown}"

LOG_FILE="/mnt/www/update/log/live_head-$(date +%Y%m%d-%H%M%S).log"
mkdir -p "$(dirname "$LOG_FILE")"

# Fonction pour tester un site
test_site() {
  local d="$1"
  read -r code time_total <<<"$(curl -sS \
    -H "Host: $d" -H "X-Forwarded-Proto: https" \
    --http1.1 --no-keepalive \
    --max-time 10 --connect-timeout 3 \
    --range 0-1024 \
    -o /dev/null \
    -w '%{http_code} %{time_total}' \
    "http://${LOCAL_IP}:${PORT}/" 2>/dev/null || echo "000 0")"
  
  # Normaliser le code 206 en 200 (Partial Content = OK)
  [[ "$code" == "206" ]] && code="200"
  
  ms="$(awk -v t="$time_total" 'BEGIN{printf("%d", t*1000)}')"
  ts="$(date +%s%3N)"
  
  printf '{"site":"%s","status":%d,"ms":%d,"pod":"%s","ts":%d}\n' \
    "$d" "$code" "$ms" "$POD_INFO" "$ts"
}

export -f test_site
export LOCAL_IP PORT POD_INFO

# Test en parall√®le (2 simultan√©s)
cd "$ROOT"
results_file="/tmp/live_head_results_$$"
ls -d *.* 2>/dev/null | xargs -P 2 -I {} bash -c 'test_site "{}"' > "$results_file"

# Construire le payload
results=()
while IFS= read -r line; do
  [[ -n "$line" ]] && results+=("$line")
done < "$results_file"
rm -f "$results_file"

# Envoi API
payload="[ $(IFS=,; echo "${results[*]}") ]"
if curl -sS -X POST "$WORKER_URL" \
  --http1.1 --no-keepalive \
  -H "Authorization: Bearer $AUTH_TOKEN" \
  -H "Content-Type: application/json" \
  -d "$payload" >/dev/null 2>&1; then
  
  echo "[$(date '+%Y-%m-%d %H:%M:%S')] OK - ${#results[@]} sites envoy√©s ($POD_INFO)" >> "$LOG_FILE"
  echo "${#results[@]} sites"
else
  echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERREUR API - ${#results[@]} sites ($POD_INFO)" >> "$LOG_FILE"
  echo "Erreur API"
fi
```

### Points cl√©s du script

- **GET avec `--range 0-1024`** : Plus rapide et fiable que HEAD (36% plus rapide, d√©tecte vraiment les erreurs)
- **Normalisation 206‚Üí200** : Le code 206 (Partial Content) est normalis√© en 200
- **Parall√©lisation avec `xargs -P 2`** : Traite 2 sites simultan√©ment pour optimiser le temps
- **D√©tection automatique du pod** : Via le fichier `/mnt/www/update/infra*-group*`
- **Logs locaux** : Stock√©s dans `/mnt/www/update/log/`

### Ex√©cution manuelle

```bash
# Rendre ex√©cutable
chmod +x /mnt/www/update/live_head.sh

# Test manuel
/mnt/www/update/live_head.sh
```

## ‚è±Ô∏è Planification (CronJob)

### Option 1: Cron syst√®me

```bash
# √âditer crontab
crontab -e

# Ajouter : scan toutes les 30 minutes
*/30 * * * * /mnt/www/update/live_head.sh >> /var/log/live_head.log 2>&1
```

### Option 2: CronJob Kubernetes

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: live-head-monitor
spec:
  schedule: "*/30 * * * *"  # Toutes les 30 minutes
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: monitor
            image: your-pod-image
            env:
            - name: WORKER_URL
              value: "https://your-worker.workers.dev/push"
            - name: AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: live-head-secret
                  key: auth-token
            command: ["/root/live_head.sh"]
          restartPolicy: OnFailure
```

## üóÑÔ∏è Base de donn√©es (D1)

### Commandes utiles

```bash
# Derni√®res entr√©es (√©tat actuel)
wrangler d1 execute live-head-db --remote --command \
"SELECT site,status,ms,location,cross,ts FROM health_last ORDER BY ts DESC LIMIT 20;"

# Historique r√©cent (si activ√©)
wrangler d1 execute live-head-db --remote --command \
"SELECT site,status,ms,ts FROM health_history ORDER BY ts DESC LIMIT 20;"

# Erreurs serveur (5xx)
wrangler d1 execute live-head-db --remote --command \
"SELECT site,status,ts FROM health_last WHERE status >= 500 ORDER BY ts DESC LIMIT 20;"

# Redirections cross-domain
wrangler d1 execute live-head-db --remote --command \
"SELECT site,status,location,ts FROM health_last WHERE cross=1 ORDER BY ts DESC LIMIT 20;"
```

### Structure des tables

| Table | Description | Colonnes principales |
|-------|-------------|---------------------|
| `health_last` | √âtat actuel de chaque site | site, status, ms, port, location, cross, ts |
| `health_history` | Historique complet (optionnel) | site, status, ms, port, location, cross, ts |

## üìä Exemples de requ√™tes SQL

Ces requ√™tes sont utilisables dans **Grafana**, **Metabase** ou tout outil BI.

### Taux d'erreurs 5xx par heure

```sql
SELECT strftime('%Y-%m-%d %H:00', ts/1000, 'unixepoch') AS heure,
       COUNT(*) AS erreurs
FROM health_history
WHERE status >= 500
GROUP BY heure
ORDER BY heure DESC;
```

### Latence moyenne par site (24h)

```sql
SELECT site, 
       AVG(ms) as latence_moyenne,
       COUNT(*) as nb_mesures
FROM health_history
WHERE ts >= strftime('%s','now','-1 day')*1000
GROUP BY site
ORDER BY latence_moyenne DESC
LIMIT 50;
```

### Top 10 sites en erreurs

```sql
SELECT site, 
       COUNT(*) as nb_erreurs,
       AVG(ms) as latence_moyenne
FROM health_history
WHERE status >= 500
GROUP BY site
ORDER BY nb_erreurs DESC
LIMIT 10;
```

### Sites avec redirections cross-domain

```sql
SELECT site,
       status,
       location,
       COUNT(*) as occurrences
FROM health_history
WHERE cross = 1
GROUP BY site, status, location
ORDER BY occurrences DESC;
```

## üõ†Ô∏è Maintenance

### D√©sactiver l'historique

```bash
# D√©finir la variable d'environnement
wrangler secret put HISTORY_ENABLED
# Entrer la valeur : 0
```

### Nettoyage automatique

```bash
# Supprimer l'historique > 30 jours
wrangler d1 execute live-head-db --remote --command \
"DELETE FROM health_history WHERE ts < strftime('%s','now','-30 days')*1000;"

# V√©rifier l'espace utilis√©
wrangler d1 execute live-head-db --remote --command \
"SELECT COUNT(*) as total_history FROM health_history;"
```

### Monitoring du Worker

```bash
# V√©rifier les logs
wrangler tail

# Statistiques d'usage
wrangler d1 execute live-head-db --remote --command \
"SELECT COUNT(*) as sites_monitores FROM health_last;"
```

## üí∞ Co√ªts

### Estimation mensuelle Cloudflare

| Service | Limite gratuite | Co√ªt d√©passement | Notre usage (30k sites, 30min) |
|---------|----------------|------------------|--------------------------------|
| **Workers** | 100k req/jour | $5/mois (10M req + 30M ms CPU) | ~5$/mois |
| **D1** | 5M lectures, 100k √©critures | $0.001/k apr√®s limite | Inclus dans gratuit |
| **KV** | ‚ùå D√©sactiv√© | $5/M √©critures | Trop cher |

**Total estim√© : 5-10 $/mois** üìä

### Comparaison alternatives

| Solution | Co√ªt mensuel (30k sites) | Avantages | Inconv√©nients |
|----------|-------------------------|-----------|---------------|
| **live_head** | $5-10 | Edge computing, faible latence | Limit√© Cloudflare |
| **Datadog** | $500-1000+ | Fonctionnalit√©s avanc√©es | Tr√®s cher |
| **UptimeRobot** | $200-500 | Interface GUI | Limit√©, pas de bulk |
| **Pingdom** | $400-800 | √âtabli, fiable | Cher, pas distribu√© |

## üîß Troubleshooting

### Probl√®mes courants

#### Worker ne r√©pond pas
```bash
# V√©rifier d√©ploiement
wrangler whoami
wrangler dev  # Test local

# V√©rifier logs
wrangler tail
```

#### Erreur d'authentification
```bash
# V√©rifier token
curl -v -H "Authorization: Bearer $AUTH_TOKEN" https://<worker>/health
```

#### Pods ne peuvent pas joindre le Worker
```bash
# Test connectivit√© r√©seau
curl -v https://<worker>/health

# V√©rifier variables d'environnement
echo $WORKER_URL $AUTH_TOKEN
```

#### Base de donn√©es pleine
```bash
# V√©rifier usage D1
wrangler d1 info live-head-db

# Nettoyer historique
wrangler d1 execute live-head-db --remote --command \
"DELETE FROM health_history WHERE ts < strftime('%s','now','-7 days')*1000;"
```

### Debug du script

```bash
# Mode debug
set -x
/root/live_head.sh

# V√©rifier un site sp√©cifique
probe_one "example.com"
echo "$payload"
```

---

## ü§ù Contributing

Les contributions sont les bienvenues ! Voici comment aider :

1. **Fork** le projet
2. **Cr√©er** une branche feature (`git checkout -b feature/amazing-feature`)
3. **Commit** vos changements (`git commit -m 'Add amazing feature'`)
4. **Push** vers la branche (`git push origin feature/amazing-feature`)
5. **Ouvrir** une Pull Request

## üìÑ License

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

---

<div align="center">
  <sub>Built with ‚ù§Ô∏è for monitoring 30k+ WordPress sites</sub>
</div>
